{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Meeting Date: 12/13/2017\n",
    "## Goals: \n",
    "1. Get introduced to machine learning terminologies.\n",
    "2. The steps involved in building any machine learning model\n",
    "3. Problems involved in applying machine learning\n",
    "\n",
    "Study Resources:\n",
    "1. [Machine Learning - A Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/pml-intro-5nov11.pdf)\n",
    "2. [Hands On Machine Learning](https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch01.html) (Requires access to safari books online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Goal of Machine Learning\n",
    "The goal of machine learning is to develop methods that\n",
    "can automatically detect patterns in data, and then to use the uncovered patterns to predict\n",
    "future data or other outcomes of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "Fields Ripe for applying Machine Learning(includes but not limited to)\n",
    "1. Computer Vision\n",
    "2. Robotics\n",
    "3. Text Processing(Natural Language Processing)\n",
    "\n",
    "Almost any field that you can think of right now, has machine learning applied to it in one form or the other. Although the complexity of the algorithms involved in them may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is needed?\n",
    "Machine Learning is less fun without understanding the math behind the algorithms. Although, it is not the objective of this study group to delve into the math, it certainly can act as a helpful resource for motivated people.\n",
    "\n",
    "In that vein, here are the math topics that you might want to start learning.\n",
    "1. Multivariate Calculus\n",
    "2. Probability\n",
    "3. Linear Algebra\n",
    "4. Basic Computer Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at some of the Machine Learning terms by building a machine learning model\n",
    "1. Dataset\n",
    "2. Features\n",
    "3. Labels\n",
    "4. Classification vs Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Sample dataset\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# Dataset - D\n",
    "D = fetch_mldata(\"MNIST original\", data_home=\"../data\") #Takes a long time. TODO: Bypass this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Features - Attributes of the input data(aka Attributes or Covariates)\n",
    "2. Labels - Target(aka Result/Truth)\n",
    "3. Common Convention is to call features as X and labels as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = D[\"data\"]\n",
    "y = D[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shape (70000, 784)\n",
      "Target Matrix shape (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Shape\", X.shape)\n",
    "print(\"Target Matrix shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing a single input of shape(1,784).\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Viewing a single input of shape(1,784).\\n\", X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. What can you infer from this sample datapoint? Try to infer as many patterns/characteristics/attributes as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for the same input 0.0\n",
      "Labels for some more input [ 0.  0.  0. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label for the same input\", y[0])\n",
    "print(\"Labels for some more input\", y[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Numbers are hard to interpret by humans but machines prefer to interpret numbers\n",
    "### 2. A very common, important and most time consuming first step to build any machine learning model is to analyze the data to make reasonable inferences about the data we are working with. Visual Analysis is often preferred as a first step because humans can quickly interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15963940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(img):\n",
    "    some_digit = img\n",
    "    some_digit_image = some_digit.reshape(28, 28)\n",
    "    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_image(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set\n",
    "Usually a portion of the dataset(D) that is reserved for training the ML model.\n",
    "## Test Set\n",
    "Usually a portion of the dataset(D) that is reserved for testing the ML model.\n",
    "\n",
    "Note: \n",
    "1. Training Set and Test set should be mutually exclusive.\n",
    "2. There are different ways to prepare this training set and test set. Most common one is called cross validation folds. We will discuss this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "1. What is the size of this training data and target?\n",
    "2. What is the size of the test data and target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification vs Regression\n",
    "1. Output Variables can in principle be anything, but if they fall within a finite set of classes - we call it a categorial variable or nominal variable - Classfication\n",
    "2. If the output variable is real values then the machine learning model is called a Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "1. What do we have here? Classification Problem or Regression Problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Earlier we had mentioned that there are multiple ways to prepare the train data and test data.\n",
    "#What we have below is the most naive way. \n",
    "#A random shuffle.\n",
    "shuffle_index = np.random.permutation(60000) #Shuffle numbers 0 to 59999\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index] #Shuffle the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. Why do we have to shuffle the data?\n",
    "2. You see the training data shuffled here. Should we shuffle the test_data too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets the following learn from building a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classifier\n",
    "2. Binary Classifiers\n",
    "3. MultiClass Classifier.\n",
    "3. MultiLabel Classifier\n",
    "4. MultiOutput Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. Just by looking at the names above, can you guess what's the difference in the output produced by these classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning model\n",
    "The term \"model\" is one of the overused and least described terms in ML. A Model is a system that has been \"trained\" to detect patterns from a dataset and is ready to make predictions on any input.\n",
    "\n",
    "Lets look at a very simple, hand wavy version of model.\n",
    "\n",
    "You might have seen posts in so many useless social media sites like the one shown below:\n",
    "\n",
    "If f(9, 3) = 21, f(6,9) = 21, f(3, 8) = 14, What is f(4,5)?\n",
    "The human mind is so fast to recognize the pattern from the three data points and three target values, our human mind is so quick to recognize the pattern and predict the answer 13. It also correctly \"learns\" that the underlying function producing the pattern is \"f(x,y) = 2x + y\".\n",
    "\n",
    "Think of the ML model as something that can learn this on a really huge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier #Stochastic Gradient Classifier. Don't worry about this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6801.30, NNZs: 617, Bias: -153.117715, T: 60000, Avg. loss: 37420.485026\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4222.04, NNZs: 634, Bias: -167.458963, T: 120000, Avg. loss: 5183.638390\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3291.77, NNZs: 641, Bias: -175.614238, T: 180000, Avg. loss: 2816.025641\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2704.40, NNZs: 642, Bias: -180.798345, T: 240000, Avg. loss: 1984.087809\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2333.12, NNZs: 647, Bias: -184.824344, T: 300000, Avg. loss: 1516.939730\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4627.12, NNZs: 579, Bias: -20.102713, T: 60000, Avg. loss: 18909.146168\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2990.30, NNZs: 606, Bias: -23.759083, T: 120000, Avg. loss: 3016.850642\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2270.97, NNZs: 614, Bias: -25.017077, T: 180000, Avg. loss: 1681.760200\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1888.88, NNZs: 622, Bias: -26.440826, T: 240000, Avg. loss: 1183.096303\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1641.68, NNZs: 627, Bias: -27.301028, T: 300000, Avg. loss: 898.309059\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7242.01, NNZs: 647, Bias: -160.446553, T: 60000, Avg. loss: 72916.171120\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4200.67, NNZs: 654, Bias: -174.531952, T: 120000, Avg. loss: 10750.262657\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3030.49, NNZs: 661, Bias: -181.874132, T: 180000, Avg. loss: 6168.272918\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2509.07, NNZs: 661, Bias: -187.438875, T: 240000, Avg. loss: 4441.826050\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2117.26, NNZs: 662, Bias: -191.672790, T: 300000, Avg. loss: 3456.155903\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6360.27, NNZs: 607, Bias: -239.726291, T: 60000, Avg. loss: 91242.318814\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3877.02, NNZs: 621, Bias: -269.584905, T: 120000, Avg. loss: 14275.643479\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2890.09, NNZs: 621, Bias: -285.376960, T: 180000, Avg. loss: 8212.559450\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2354.91, NNZs: 622, Bias: -297.182840, T: 240000, Avg. loss: 5929.879458\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1974.46, NNZs: 624, Bias: -305.670930, T: 300000, Avg. loss: 4513.085922\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6573.88, NNZs: 646, Bias: -88.405304, T: 60000, Avg. loss: 54794.194228\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4274.57, NNZs: 651, Bias: -96.164138, T: 120000, Avg. loss: 7599.020132\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.89, NNZs: 662, Bias: -101.451740, T: 180000, Avg. loss: 4396.601606\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2655.00, NNZs: 667, Bias: -104.778115, T: 240000, Avg. loss: 2913.299354\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2288.79, NNZs: 668, Bias: -107.553633, T: 300000, Avg. loss: 2334.334407\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7839.49, NNZs: 625, Bias: 41.422495, T: 60000, Avg. loss: 96951.960812\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4720.22, NNZs: 636, Bias: 49.491579, T: 120000, Avg. loss: 13586.475544\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3670.99, NNZs: 641, Bias: 54.023536, T: 180000, Avg. loss: 8000.413419\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3037.70, NNZs: 646, Bias: 58.108423, T: 240000, Avg. loss: 5527.279482\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2560.39, NNZs: 649, Bias: 60.446801, T: 300000, Avg. loss: 4232.201799\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6084.32, NNZs: 591, Bias: -98.549158, T: 60000, Avg. loss: 42544.375546\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3677.28, NNZs: 598, Bias: -114.275195, T: 120000, Avg. loss: 6919.356026\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2735.69, NNZs: 600, Bias: -123.245485, T: 180000, Avg. loss: 3967.649019\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2192.25, NNZs: 608, Bias: -129.551508, T: 240000, Avg. loss: 2830.065566\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1897.95, NNZs: 608, Bias: -134.624523, T: 300000, Avg. loss: 2144.817369\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5454.29, NNZs: 600, Bias: -25.912582, T: 60000, Avg. loss: 43127.253504\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3347.66, NNZs: 615, Bias: -29.040390, T: 120000, Avg. loss: 6789.630621\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2492.93, NNZs: 616, Bias: -29.957684, T: 180000, Avg. loss: 4002.593446\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2058.54, NNZs: 627, Bias: -30.745959, T: 240000, Avg. loss: 2706.681191\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1779.46, NNZs: 629, Bias: -31.688652, T: 300000, Avg. loss: 2044.069239\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7120.95, NNZs: 628, Bias: -682.953703, T: 60000, Avg. loss: 165897.132455\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4465.79, NNZs: 643, Bias: -765.854401, T: 120000, Avg. loss: 27086.379542\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3350.74, NNZs: 645, Bias: -814.813051, T: 180000, Avg. loss: 15678.451571\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2677.97, NNZs: 645, Bias: -847.875693, T: 240000, Avg. loss: 11234.971875\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2277.76, NNZs: 651, Bias: -874.283834, T: 300000, Avg. loss: 8667.789639\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7096.94, NNZs: 636, Bias: -250.388409, T: 60000, Avg. loss: 113542.246679\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4270.82, NNZs: 646, Bias: -284.929336, T: 120000, Avg. loss: 17862.120795\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3154.83, NNZs: 654, Bias: -305.843368, T: 180000, Avg. loss: 10350.557263\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2564.59, NNZs: 660, Bias: -320.387277, T: 240000, Avg. loss: 7354.594881\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2154.86, NNZs: 660, Bias: -332.176250, T: 300000, Avg. loss: 5662.941943\n",
      "Total training time: 0.41 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=13, shuffle=True,\n",
       "       tol=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "sgd = SGDClassifier(random_state=13, verbose=1)\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at all the parameters to a typical ML model\n",
    "The above steps spits out a buunch of words that are new and alien.\n",
    "\n",
    "They are ordered in terms of importance. Items marked as * requires a deeper understanding of what happens inside attempts to learn model.\n",
    "1. Loss - A parameter to measure \"how far off\" you are from the ground truth. A model can learn nothing if doesn't have an understanding how to \"measure\" itself. If you go back to the analogy described earlier with f(x,y) = 2x + y, your mind arrived at this solution after trying a couple of functions like this f(x, y) = 7y. This function works for datapoint 1- f(7,3) but after you look at the second datapoint f(6,9), your mind correctly excluded this function from consideration because 7(9) = 63. The loss = 63 - 21 = 42 is so large.\n",
    "2. Norm* - Normalization Term (aka Regularization term, Penalty).\n",
    "3. Bias* - Bias Term\n",
    "4. Epoch - Full iteration of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happended behind the scenes?\n",
    "1. A model got trained. How?\n",
    "2. It took the training dataset(60000 samples)\n",
    "3. Divided them into batches of arbitrary size(batch_size usually a power of 2. Ex:128)\n",
    "4. It looked at a batch of data(say 128 examples and its corresponding ground truth value)\n",
    "5. Tried to learn a function(aka decision function) - Just like how your mind learned a function.\n",
    "6. Looked at another set of data(next batch 128 samples - Tried to apply the function that it learned. Evaluates itself using the loss function that you passed to it. Based on the loss function, it corrects the learned function by fine tuning its hyperparameters.\n",
    "7. Moves to the next data set until it finishes the entire training dataset.\n",
    "8. This process of iterating though the entire training set in batches is called one Epoch.\n",
    "9. After an epoch, you evaluate the model if it meets the terminating condition.\n",
    "10. If it meets the terminating condition, exit. If not shuffle the data and repeat.\n",
    "11. Terminating condition can be something as simple as an arbitrary number of epochs(say 50) or you can use a evaluation function like RMSE(Root Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. What is a hyperparameter? Try to think of this using the simpler example f(x,y) = 2x + y\n",
    "2. Why is it important to shuffle the data after en epoch?\n",
    "3. Step 9 says we should evaluate our model before calling the epoch done. On what data should we evaluate our model? Try to think why training/test can/can't be used for the evaluation purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try to see if our model anything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a210bbcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(X_test[5000]) #Pick an arbitrary image from the dataset and visualize it so we know what to expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alight, my trained ML model, can you make a prediction for me?\n",
    "sgd.predict(X_test[5000].reshape(1,-1)) #Randomly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me: Alright, my ML model you say that it's a 4 but if you meet my professors who evaluate my exam papers they'd say that I write my 9 that way. #HumansVsAI\n",
    "\n",
    "AI: No. It's a 4. Have a look at the ground truth values if you have any doubts.\n",
    "\n",
    "Me: Alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me: &lt;Insert your favorite man beating machine with a baseball bat meme>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try to evaluate our ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 7957.05, NNZs: 614, Bias: -166.090636, T: 47995, Avg. loss: 51435.649121\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4954.34, NNZs: 625, Bias: -180.749789, T: 95990, Avg. loss: 6512.361527\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3825.52, NNZs: 629, Bias: -189.162078, T: 143985, Avg. loss: 3708.960640\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3212.69, NNZs: 632, Bias: -194.264126, T: 191980, Avg. loss: 2471.820757\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2739.28, NNZs: 633, Bias: -198.141969, T: 239975, Avg. loss: 1919.245649\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5821.99, NNZs: 577, Bias: -19.425477, T: 47995, Avg. loss: 24299.635070\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3600.82, NNZs: 594, Bias: -23.314097, T: 95990, Avg. loss: 3595.182455\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2724.11, NNZs: 602, Bias: -24.475681, T: 143985, Avg. loss: 2008.292873\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2230.31, NNZs: 610, Bias: -25.563676, T: 191980, Avg. loss: 1462.399915\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1909.66, NNZs: 614, Bias: -26.117937, T: 239975, Avg. loss: 1028.209255\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8023.53, NNZs: 626, Bias: -172.784505, T: 47995, Avg. loss: 86012.643556\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4974.17, NNZs: 628, Bias: -185.389042, T: 95990, Avg. loss: 13526.414425\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3596.33, NNZs: 639, Bias: -191.905794, T: 143985, Avg. loss: 7629.175984\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2928.62, NNZs: 642, Bias: -197.211073, T: 191980, Avg. loss: 5635.091911\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2495.35, NNZs: 647, Bias: -201.126935, T: 239975, Avg. loss: 4223.558791\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7899.60, NNZs: 605, Bias: -217.889872, T: 47995, Avg. loss: 113368.068248\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4689.77, NNZs: 609, Bias: -244.509653, T: 95990, Avg. loss: 18419.441718\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3572.84, NNZs: 620, Bias: -260.389887, T: 143985, Avg. loss: 10451.030470\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2881.93, NNZs: 621, Bias: -270.849623, T: 191980, Avg. loss: 7324.164295\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2445.17, NNZs: 623, Bias: -279.712765, T: 239975, Avg. loss: 5725.901024\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7987.35, NNZs: 635, Bias: -77.542597, T: 47995, Avg. loss: 64164.124275\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4953.54, NNZs: 653, Bias: -87.539464, T: 95990, Avg. loss: 9473.956253\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3778.60, NNZs: 657, Bias: -93.147778, T: 143985, Avg. loss: 5345.257416\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3157.13, NNZs: 662, Bias: -96.314180, T: 191980, Avg. loss: 3741.667579\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2732.91, NNZs: 668, Bias: -99.247856, T: 239975, Avg. loss: 2977.957831\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8949.52, NNZs: 623, Bias: 27.615831, T: 47995, Avg. loss: 106711.801808\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5467.74, NNZs: 636, Bias: 35.004550, T: 95990, Avg. loss: 17061.154149\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4228.55, NNZs: 641, Bias: 40.603815, T: 143985, Avg. loss: 9797.252997\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3494.65, NNZs: 647, Bias: 44.411144, T: 191980, Avg. loss: 6924.218653\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3020.47, NNZs: 647, Bias: 47.543192, T: 239975, Avg. loss: 5235.287166\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7466.63, NNZs: 609, Bias: -134.999699, T: 47995, Avg. loss: 51741.623283\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4359.71, NNZs: 610, Bias: -150.556338, T: 95990, Avg. loss: 8265.389567\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3269.52, NNZs: 612, Bias: -159.026560, T: 143985, Avg. loss: 4914.143012\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2734.43, NNZs: 614, Bias: -165.449891, T: 191980, Avg. loss: 3295.734225\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2272.48, NNZs: 614, Bias: -169.923542, T: 239975, Avg. loss: 2557.766857\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6549.15, NNZs: 589, Bias: -29.710165, T: 47995, Avg. loss: 53555.411688\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3960.04, NNZs: 602, Bias: -31.505404, T: 95990, Avg. loss: 8377.580364\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2982.27, NNZs: 605, Bias: -32.838403, T: 143985, Avg. loss: 4806.324911\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2489.36, NNZs: 616, Bias: -34.242724, T: 191980, Avg. loss: 3356.883030\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2111.59, NNZs: 618, Bias: -34.574494, T: 239975, Avg. loss: 2594.199847\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9179.31, NNZs: 616, Bias: -621.434573, T: 47995, Avg. loss: 199930.897564\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5481.78, NNZs: 622, Bias: -705.867129, T: 95990, Avg. loss: 33664.100279\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3900.27, NNZs: 631, Bias: -753.782746, T: 143985, Avg. loss: 19884.115855\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3214.11, NNZs: 638, Bias: -787.841544, T: 191980, Avg. loss: 13810.179972\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2677.18, NNZs: 641, Bias: -813.028603, T: 239975, Avg. loss: 10914.722947\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8437.25, NNZs: 622, Bias: -252.963436, T: 47995, Avg. loss: 130656.797417\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5134.05, NNZs: 642, Bias: -289.515128, T: 95990, Avg. loss: 22021.561531\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3821.04, NNZs: 658, Bias: -310.391395, T: 143985, Avg. loss: 12701.010367\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3115.58, NNZs: 663, Bias: -324.925111, T: 191980, Avg. loss: 8956.095772\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2608.10, NNZs: 664, Bias: -335.878167, T: 239975, Avg. loss: 7071.748282\n",
      "Total training time: 0.28 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.8s finished\n",
      "/anaconda2/envs/ml3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 7684.62, NNZs: 614, Bias: -177.343446, T: 47998, Avg. loss: 48726.966291\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4907.80, NNZs: 626, Bias: -191.657338, T: 95996, Avg. loss: 6377.986202\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3769.59, NNZs: 629, Bias: -199.702117, T: 143994, Avg. loss: 3450.860659\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3125.56, NNZs: 635, Bias: -204.644049, T: 191992, Avg. loss: 2319.778924\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2726.74, NNZs: 637, Bias: -208.647697, T: 239990, Avg. loss: 1799.708363\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5821.92, NNZs: 577, Bias: -52.949635, T: 47998, Avg. loss: 26780.386832\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3670.32, NNZs: 598, Bias: -56.951265, T: 95996, Avg. loss: 3851.249344\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2762.46, NNZs: 602, Bias: -59.256502, T: 143994, Avg. loss: 2194.531382\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2217.08, NNZs: 612, Bias: -59.999990, T: 191992, Avg. loss: 1478.060653\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1934.51, NNZs: 623, Bias: -60.939006, T: 239990, Avg. loss: 1126.482930\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7887.95, NNZs: 647, Bias: -160.268654, T: 47998, Avg. loss: 80966.047153\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4866.26, NNZs: 650, Bias: -174.730178, T: 95996, Avg. loss: 13598.703678\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3508.91, NNZs: 658, Bias: -181.422289, T: 143994, Avg. loss: 7926.068139\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2876.26, NNZs: 665, Bias: -187.604515, T: 191992, Avg. loss: 5551.489752\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2495.68, NNZs: 667, Bias: -191.962692, T: 239990, Avg. loss: 4295.672988\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8034.63, NNZs: 605, Bias: -215.263825, T: 47998, Avg. loss: 106630.647544\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4799.47, NNZs: 617, Bias: -243.081457, T: 95996, Avg. loss: 17515.766734\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3513.33, NNZs: 621, Bias: -258.823870, T: 143994, Avg. loss: 10071.350350\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2802.99, NNZs: 622, Bias: -270.061425, T: 191992, Avg. loss: 7161.245267\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2395.99, NNZs: 625, Bias: -279.063350, T: 239990, Avg. loss: 5559.889278\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7873.01, NNZs: 635, Bias: -28.261151, T: 47998, Avg. loss: 65853.135664\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5006.14, NNZs: 655, Bias: -38.164017, T: 95996, Avg. loss: 9287.938433\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3884.56, NNZs: 660, Bias: -43.659658, T: 143994, Avg. loss: 5318.577803\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3210.12, NNZs: 660, Bias: -47.721049, T: 191992, Avg. loss: 3833.633137\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2742.86, NNZs: 661, Bias: -50.219082, T: 239990, Avg. loss: 2912.806822\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9691.04, NNZs: 618, Bias: 38.642285, T: 47998, Avg. loss: 113531.007111\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5835.68, NNZs: 631, Bias: 46.998797, T: 95996, Avg. loss: 16453.548619\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4430.52, NNZs: 636, Bias: 51.199187, T: 143994, Avg. loss: 9705.917781\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3662.02, NNZs: 639, Bias: 54.490965, T: 191992, Avg. loss: 6735.151944\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3118.51, NNZs: 640, Bias: 57.187184, T: 239990, Avg. loss: 5287.890150\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7254.37, NNZs: 584, Bias: -151.214947, T: 47998, Avg. loss: 53320.789405\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4479.57, NNZs: 599, Bias: -166.930992, T: 95996, Avg. loss: 8487.633267\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3283.20, NNZs: 602, Bias: -175.620894, T: 143994, Avg. loss: 4850.214208\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2676.91, NNZs: 605, Bias: -181.766837, T: 191992, Avg. loss: 3469.249176\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2249.68, NNZs: 607, Bias: -186.745915, T: 239990, Avg. loss: 2813.757641\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6566.43, NNZs: 597, Bias: -64.531268, T: 47998, Avg. loss: 52564.020086\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3965.95, NNZs: 610, Bias: -67.612859, T: 95996, Avg. loss: 8290.151327\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3136.83, NNZs: 611, Bias: -69.216092, T: 143994, Avg. loss: 4728.087206\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2514.82, NNZs: 614, Bias: -69.430076, T: 191992, Avg. loss: 3343.042710\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2179.70, NNZs: 625, Bias: -69.695687, T: 239990, Avg. loss: 2521.086724\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9211.93, NNZs: 633, Bias: -634.300948, T: 47998, Avg. loss: 206473.999596\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5375.51, NNZs: 645, Bias: -720.055828, T: 95996, Avg. loss: 34222.904213\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3936.03, NNZs: 654, Bias: -768.246110, T: 143994, Avg. loss: 19821.657257\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3174.32, NNZs: 665, Bias: -802.342167, T: 191992, Avg. loss: 14013.276185\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2735.50, NNZs: 665, Bias: -827.786704, T: 239990, Avg. loss: 10834.882964\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8555.86, NNZs: 637, Bias: -268.367945, T: 47998, Avg. loss: 127113.877165\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5183.88, NNZs: 647, Bias: -304.410928, T: 95996, Avg. loss: 21631.328025\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3808.31, NNZs: 657, Bias: -324.320594, T: 143994, Avg. loss: 12608.703596\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3096.52, NNZs: 660, Bias: -338.798973, T: 191992, Avg. loss: 9036.801028\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2635.82, NNZs: 664, Bias: -349.575706, T: 239990, Avg. loss: 6957.646046\n",
      "Total training time: 0.29 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.9s finished\n",
      "/anaconda2/envs/ml3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 8036.06, NNZs: 619, Bias: -138.194909, T: 48000, Avg. loss: 47751.339790\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5053.73, NNZs: 626, Bias: -153.539113, T: 96000, Avg. loss: 6272.438419\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3864.83, NNZs: 628, Bias: -161.370085, T: 144000, Avg. loss: 3680.265347\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3212.26, NNZs: 633, Bias: -166.993017, T: 192000, Avg. loss: 2546.745245\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2715.94, NNZs: 636, Bias: -170.979278, T: 240000, Avg. loss: 1972.343760\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5720.14, NNZs: 564, Bias: -32.048773, T: 48000, Avg. loss: 26514.140551\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3533.00, NNZs: 589, Bias: -35.889479, T: 96000, Avg. loss: 3909.226137\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2703.57, NNZs: 599, Bias: -37.633221, T: 144000, Avg. loss: 2255.190147\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2205.59, NNZs: 606, Bias: -38.841557, T: 192000, Avg. loss: 1500.501098\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1903.78, NNZs: 615, Bias: -39.324794, T: 240000, Avg. loss: 1223.852825\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7696.07, NNZs: 624, Bias: -136.943553, T: 48000, Avg. loss: 87931.058339\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4813.50, NNZs: 648, Bias: -151.179469, T: 96000, Avg. loss: 13061.994967\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3595.90, NNZs: 655, Bias: -159.259718, T: 144000, Avg. loss: 7841.167753\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2961.80, NNZs: 658, Bias: -164.688175, T: 192000, Avg. loss: 5468.954294\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2565.79, NNZs: 658, Bias: -168.780147, T: 240000, Avg. loss: 4228.028870\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7891.31, NNZs: 602, Bias: -235.287268, T: 48000, Avg. loss: 104379.002615\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4709.99, NNZs: 610, Bias: -262.381500, T: 96000, Avg. loss: 17293.397419\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3480.62, NNZs: 615, Bias: -280.517674, T: 144000, Avg. loss: 10495.455040\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2789.78, NNZs: 615, Bias: -292.567994, T: 192000, Avg. loss: 7335.329761\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2418.64, NNZs: 620, Bias: -301.116899, T: 240000, Avg. loss: 5599.630081\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7725.82, NNZs: 630, Bias: -118.855008, T: 48000, Avg. loss: 60050.933149\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5145.61, NNZs: 643, Bias: -129.759272, T: 96000, Avg. loss: 9233.757683\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3922.27, NNZs: 648, Bias: -134.925053, T: 144000, Avg. loss: 5271.079937\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3288.92, NNZs: 658, Bias: -138.835514, T: 192000, Avg. loss: 3833.974777\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2754.33, NNZs: 661, Bias: -140.876082, T: 240000, Avg. loss: 2901.454096\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9305.51, NNZs: 615, Bias: 24.965124, T: 48000, Avg. loss: 115779.173381\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5816.66, NNZs: 629, Bias: 32.884009, T: 96000, Avg. loss: 16695.624136\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4375.22, NNZs: 644, Bias: 37.832779, T: 144000, Avg. loss: 9491.823847\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3496.83, NNZs: 645, Bias: 41.551382, T: 192000, Avg. loss: 6716.369644\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2957.10, NNZs: 648, Bias: 43.666513, T: 240000, Avg. loss: 5184.999136\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7153.20, NNZs: 578, Bias: -156.194474, T: 48000, Avg. loss: 49821.248289\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4300.94, NNZs: 596, Bias: -172.375488, T: 96000, Avg. loss: 8311.257192\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3175.08, NNZs: 602, Bias: -181.034554, T: 144000, Avg. loss: 4892.366771\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2661.15, NNZs: 602, Bias: -186.638222, T: 192000, Avg. loss: 3262.305069\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2275.18, NNZs: 602, Bias: -191.505157, T: 240000, Avg. loss: 2657.766862\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6996.26, NNZs: 594, Bias: -32.716982, T: 48000, Avg. loss: 50895.189593\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4125.70, NNZs: 611, Bias: -35.259798, T: 96000, Avg. loss: 8288.587491\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3151.58, NNZs: 624, Bias: -36.385274, T: 144000, Avg. loss: 4755.610734\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2500.17, NNZs: 628, Bias: -37.793758, T: 192000, Avg. loss: 3338.085025\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2124.84, NNZs: 632, Bias: -37.819049, T: 240000, Avg. loss: 2534.797271\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9117.40, NNZs: 624, Bias: -586.959450, T: 48000, Avg. loss: 191021.307027\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5432.78, NNZs: 637, Bias: -668.401203, T: 96000, Avg. loss: 33555.049331\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4059.94, NNZs: 642, Bias: -716.279016, T: 144000, Avg. loss: 19677.197515\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3279.35, NNZs: 644, Bias: -748.937038, T: 192000, Avg. loss: 13959.136827\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2799.37, NNZs: 650, Bias: -774.517733, T: 240000, Avg. loss: 10743.729778\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8469.99, NNZs: 635, Bias: -323.804805, T: 48000, Avg. loss: 142334.559434\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5055.46, NNZs: 646, Bias: -360.936529, T: 96000, Avg. loss: 21837.662403\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3732.14, NNZs: 651, Bias: -382.703720, T: 144000, Avg. loss: 12778.945090\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3007.47, NNZs: 655, Bias: -397.111832, T: 192000, Avg. loss: 9128.790046\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2590.82, NNZs: 659, Bias: -408.064641, T: 240000, Avg. loss: 7072.818018\n",
      "Total training time: 0.30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.8s finished\n",
      "/anaconda2/envs/ml3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 7861.06, NNZs: 615, Bias: -166.499010, T: 48003, Avg. loss: 44470.296757\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5083.21, NNZs: 636, Bias: -180.428411, T: 96006, Avg. loss: 5885.740441\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3822.99, NNZs: 637, Bias: -188.162620, T: 144009, Avg. loss: 3378.420722\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3179.40, NNZs: 641, Bias: -193.707818, T: 192012, Avg. loss: 2284.830592\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2774.27, NNZs: 644, Bias: -197.987980, T: 240015, Avg. loss: 1807.757155\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5701.32, NNZs: 560, Bias: -65.354483, T: 48003, Avg. loss: 26307.257721\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3591.69, NNZs: 590, Bias: -70.595075, T: 96006, Avg. loss: 3588.973728\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2648.85, NNZs: 603, Bias: -72.212031, T: 144009, Avg. loss: 1955.757309\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2276.59, NNZs: 610, Bias: -73.717586, T: 192012, Avg. loss: 1284.656685\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1978.24, NNZs: 615, Bias: -74.591167, T: 240015, Avg. loss: 1063.520150\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8214.38, NNZs: 641, Bias: -192.930415, T: 48003, Avg. loss: 92460.601946\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4821.40, NNZs: 650, Bias: -206.304470, T: 96006, Avg. loss: 13125.924339\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3568.63, NNZs: 653, Bias: -213.555789, T: 144009, Avg. loss: 7572.831026\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3013.52, NNZs: 656, Bias: -218.509371, T: 192012, Avg. loss: 5346.623874\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2499.72, NNZs: 658, Bias: -222.212274, T: 240015, Avg. loss: 4297.177321\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7762.66, NNZs: 602, Bias: -249.217465, T: 48003, Avg. loss: 103911.929068\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4562.36, NNZs: 614, Bias: -278.598563, T: 96006, Avg. loss: 18054.010133\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3386.28, NNZs: 620, Bias: -295.395899, T: 144009, Avg. loss: 10440.570805\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2741.67, NNZs: 622, Bias: -306.396149, T: 192012, Avg. loss: 7403.165634\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2412.60, NNZs: 622, Bias: -316.378274, T: 240015, Avg. loss: 5599.941089\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8348.99, NNZs: 643, Bias: -94.292443, T: 48003, Avg. loss: 62186.727782\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4971.22, NNZs: 649, Bias: -104.340095, T: 96006, Avg. loss: 9187.594913\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3877.03, NNZs: 659, Bias: -109.413725, T: 144009, Avg. loss: 5526.219649\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3167.18, NNZs: 662, Bias: -112.309066, T: 192012, Avg. loss: 3818.275837\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2713.36, NNZs: 664, Bias: -114.972336, T: 240015, Avg. loss: 2898.638269\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9288.46, NNZs: 623, Bias: 53.615908, T: 48003, Avg. loss: 109189.471357\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5609.24, NNZs: 629, Bias: 61.967432, T: 96006, Avg. loss: 17017.331810\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4279.19, NNZs: 633, Bias: 67.318654, T: 144009, Avg. loss: 9751.326274\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3452.19, NNZs: 637, Bias: 70.786491, T: 192012, Avg. loss: 6913.264441\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2987.41, NNZs: 638, Bias: 73.290137, T: 240015, Avg. loss: 5279.105397\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7443.34, NNZs: 576, Bias: -157.005300, T: 48003, Avg. loss: 54912.093851\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4311.37, NNZs: 585, Bias: -173.158459, T: 96006, Avg. loss: 8448.097571\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3270.73, NNZs: 589, Bias: -183.024518, T: 144009, Avg. loss: 5069.635266\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2661.38, NNZs: 597, Bias: -189.941828, T: 192012, Avg. loss: 3561.196608\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2259.12, NNZs: 599, Bias: -195.145846, T: 240015, Avg. loss: 2726.768729\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6783.41, NNZs: 608, Bias: -32.588968, T: 48003, Avg. loss: 52788.617770\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4077.80, NNZs: 613, Bias: -34.852434, T: 96006, Avg. loss: 8212.761985\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3129.64, NNZs: 622, Bias: -36.719524, T: 144009, Avg. loss: 4617.792059\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2530.34, NNZs: 628, Bias: -37.220758, T: 192012, Avg. loss: 3420.919815\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2145.22, NNZs: 633, Bias: -37.429644, T: 240015, Avg. loss: 2555.333729\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9109.03, NNZs: 628, Bias: -606.419756, T: 48003, Avg. loss: 195896.325704\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5532.93, NNZs: 636, Bias: -688.445953, T: 96006, Avg. loss: 33256.050025\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3931.06, NNZs: 644, Bias: -736.547018, T: 144009, Avg. loss: 19223.413305\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3208.55, NNZs: 656, Bias: -770.153855, T: 192012, Avg. loss: 13740.974446\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2729.12, NNZs: 664, Bias: -795.707683, T: 240015, Avg. loss: 10579.839803\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8257.45, NNZs: 636, Bias: -253.146798, T: 48003, Avg. loss: 133978.146729\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5038.03, NNZs: 650, Bias: -293.048090, T: 96006, Avg. loss: 22089.836052\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3676.31, NNZs: 656, Bias: -313.184792, T: 144009, Avg. loss: 12850.709025\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3022.54, NNZs: 659, Bias: -328.558295, T: 192012, Avg. loss: 9191.110083\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2595.10, NNZs: 660, Bias: -340.295348, T: 240015, Avg. loss: 7161.067871\n",
      "Total training time: 0.29 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.9s finished\n",
      "/anaconda2/envs/ml3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 7899.75, NNZs: 596, Bias: -165.597139, T: 48004, Avg. loss: 41367.672921\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4923.56, NNZs: 623, Bias: -180.325307, T: 96008, Avg. loss: 6409.546572\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3781.25, NNZs: 628, Bias: -187.806200, T: 144012, Avg. loss: 3587.717981\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3164.62, NNZs: 630, Bias: -193.237066, T: 192016, Avg. loss: 2455.060674\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2737.17, NNZs: 633, Bias: -196.880900, T: 240020, Avg. loss: 1875.842576\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6107.77, NNZs: 554, Bias: -28.924151, T: 48004, Avg. loss: 27154.998363\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3695.06, NNZs: 587, Bias: -32.142625, T: 96008, Avg. loss: 3662.676862\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2775.62, NNZs: 591, Bias: -33.850247, T: 144012, Avg. loss: 2041.087811\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2271.23, NNZs: 600, Bias: -34.504331, T: 192016, Avg. loss: 1479.018839\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1972.94, NNZs: 603, Bias: -35.434646, T: 240020, Avg. loss: 1123.967622\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7861.85, NNZs: 639, Bias: -138.777968, T: 48004, Avg. loss: 87214.433278\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4968.26, NNZs: 641, Bias: -152.673846, T: 96008, Avg. loss: 13390.714962\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3672.43, NNZs: 642, Bias: -160.937620, T: 144012, Avg. loss: 7742.913342\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3056.63, NNZs: 643, Bias: -165.572012, T: 192016, Avg. loss: 5441.900518\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2585.66, NNZs: 649, Bias: -169.623141, T: 240020, Avg. loss: 4223.017989\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8208.11, NNZs: 606, Bias: -227.963833, T: 48004, Avg. loss: 106115.747370\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4693.26, NNZs: 616, Bias: -253.237937, T: 96008, Avg. loss: 17361.918415\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3518.42, NNZs: 623, Bias: -269.678670, T: 144012, Avg. loss: 10054.220300\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2751.01, NNZs: 624, Bias: -280.289909, T: 192016, Avg. loss: 7255.196334\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2373.56, NNZs: 626, Bias: -289.926048, T: 240020, Avg. loss: 5569.735218\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7928.34, NNZs: 640, Bias: -94.789740, T: 48004, Avg. loss: 65094.016575\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5094.89, NNZs: 652, Bias: -104.221839, T: 96008, Avg. loss: 9517.683420\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3825.77, NNZs: 654, Bias: -109.064065, T: 144012, Avg. loss: 5511.380566\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3150.51, NNZs: 659, Bias: -112.491465, T: 192016, Avg. loss: 3888.786405\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2705.10, NNZs: 659, Bias: -114.789276, T: 240020, Avg. loss: 2914.453971\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9275.62, NNZs: 612, Bias: 53.587109, T: 48004, Avg. loss: 112825.545061\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5795.78, NNZs: 624, Bias: 60.055514, T: 96008, Avg. loss: 16568.606575\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4229.53, NNZs: 629, Bias: 64.773735, T: 144012, Avg. loss: 9459.976442\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3451.16, NNZs: 633, Bias: 67.558724, T: 192016, Avg. loss: 6857.315231\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2951.81, NNZs: 634, Bias: 70.234949, T: 240020, Avg. loss: 5242.244076\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7417.29, NNZs: 589, Bias: -160.166590, T: 48004, Avg. loss: 56095.713947\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4290.38, NNZs: 602, Bias: -176.480171, T: 96008, Avg. loss: 8945.860696\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3156.71, NNZs: 605, Bias: -184.834151, T: 144012, Avg. loss: 5007.202906\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2645.57, NNZs: 610, Bias: -191.678115, T: 192016, Avg. loss: 3566.826810\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2228.96, NNZs: 612, Bias: -196.780798, T: 240020, Avg. loss: 2804.058019\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7067.17, NNZs: 604, Bias: -14.962028, T: 48004, Avg. loss: 51838.390439\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4150.76, NNZs: 615, Bias: -18.410425, T: 96008, Avg. loss: 8250.067330\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3159.71, NNZs: 623, Bias: -20.150646, T: 144012, Avg. loss: 4821.867060\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2573.46, NNZs: 628, Bias: -21.324866, T: 192016, Avg. loss: 3406.635228\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2149.96, NNZs: 628, Bias: -21.552109, T: 240020, Avg. loss: 2640.778383\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8907.89, NNZs: 610, Bias: -576.179544, T: 48004, Avg. loss: 194952.926204\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5409.98, NNZs: 636, Bias: -659.737691, T: 96008, Avg. loss: 33634.830071\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4045.74, NNZs: 641, Bias: -708.086963, T: 144012, Avg. loss: 19948.471649\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3245.44, NNZs: 643, Bias: -741.526920, T: 192016, Avg. loss: 14140.414785\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2792.89, NNZs: 647, Bias: -767.548690, T: 240020, Avg. loss: 10802.514211\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8140.57, NNZs: 611, Bias: -284.865378, T: 48004, Avg. loss: 132911.295499\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4970.16, NNZs: 640, Bias: -323.740035, T: 96008, Avg. loss: 22390.029260\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3750.48, NNZs: 644, Bias: -346.757633, T: 144012, Avg. loss: 13008.150040\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3098.35, NNZs: 652, Bias: -361.577050, T: 192016, Avg. loss: 9107.543226\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2595.91, NNZs: 654, Bias: -372.962621, T: 240020, Avg. loss: 7084.439349\n",
      "Total training time: 0.29 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "## Evaluation Functions and Validation Schemes\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict ## Cross Validation. See below\n",
    "y_train_pred = cross_val_predict(sgd, X_train, y_train, cv=5) ## 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "![alt K-fold Cross Validation](https://raw.githubusercontent.com/ArulselvanMadhavan/ml-study-group/master/resources/images/k-fold-diagram.png \"K-Fold cross validation\")\n",
    "\n",
    "Note: Test Fold is also called as Validation Set in some literature.\n",
    "\n",
    "[Source:http://karlrosaen.com/ml/learning-log/2016-06-20/](http://karlrosaen.com/ml/learning-log/2016-06-20/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. Why is Cross Validation important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  5.,  1.,  5.,  7.,  3.,  1.,  9.,  4.,  2.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred[:10] ##Lets look at the predictions for the first 10 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. Can you define the notion of accuracy for our problem? When do you say a prediction is correct for a single image? How do you accumulate that notion for the entire data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy \n",
    "def accuracy(truth, predictions):\n",
    "    return (np.sum(truth == predictions) / truth.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84535000000000005"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train, y_train_pred) #Train Accuracy of around 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "It should be fairly trivial to define what they mean but can you reason about where they should be used in the lifecycle of a ML model and what reasonable inferences can be made from them?\n",
    "1. Training Set Accuracy\n",
    "2. Validation Set Accuracy\n",
    "3. Test Set Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we improve accuracy?\n",
    "\n",
    "1. As I mentioned earlier machines can interpret numbers well and humans can interpret information from arrregated data(in most cases aggregated data is represented via visual medium). It is very effective compared to just looking at numbers.\n",
    "2. One of the tools that they use to understand model performance and make reasonable tweaks to ML model is called confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5526,    2,   53,   20,   10,   30,   43,   19,  214,    6],\n",
       "       [   5, 6457,   45,    7,    8,   18,    9,   50,  134,    9],\n",
       "       [  41,   75, 5064,   79,   68,   20,  145,  125,  320,   21],\n",
       "       [  75,   72,  361, 4316,   18,  455,   37,  119,  600,   78],\n",
       "       [  20,   42,   54,    6, 5018,   26,   70,   53,  356,  197],\n",
       "       [ 101,   47,   73,  162,   90, 3811,  118,   60,  860,   99],\n",
       "       [  55,   33,  156,    1,   69,   72, 5363,   16,  147,    6],\n",
       "       [  22,   39,   53,   17,   64,   15,    7, 5735,  118,  195],\n",
       "       [  57,  211,  120,   95,   42,  114,   43,   77, 5011,   81],\n",
       "       [  33,   53,   29,   55,  187,  160,    3,  612,  397, 4420]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mx.shape #Confusion matrix is a num_of_classes x num_of_classes matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c239748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1. By just looking at the picture, can you reason why this is a handy tool to understand and tweak a ML model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work In Progress Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems in Machine Learning\n",
    "1. Long Tail Problem\n",
    "![alt text](https://github.com/ArulselvanMadhavan/ml-study-group/raw/master/resources/images/Long_Tail_Problem.jpg \"Long Tail Problem\")\n",
    "[Source: http://www.longtail.com/about.html](http://www.longtail.com/about.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. One Vs One Classifier\n",
    "2. One Vs Rest Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
